{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many were afraid of seeing the death of the erxercise of writing with the development of the Internet (as for the TV before...), it has surprisingly modernized and popularized it (blogs, comments, reviews, tweet? etc.). Text is consequently a prevalent form of information on the Internet. But how to extract information from blocs of text readable by the machine and the machine learning algorithm? Here is the passionating challenge of this projet we are now going to tackle!\n",
    "\n",
    "Quantity and accessibility of data is not much a challenge anymore. The challenge lies in the ability to produce meaningful analysis from relevant data in the right context. \n",
    "\n",
    "\"Data have quality if they satisfy the requirements of the intended use.\" says Data and Mining, Concepts and Techniques. \n",
    "\n",
    "\n",
    "Nowadays, the huge amount of data often goes along with poor quality due to noise, missing or inconsistent data for instance. It is then crucial to explore them carefully. \n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "robust\n",
    "\n",
    "As suggested by the project description, we can try to extract additional information from \n",
    "\n",
    "From the data provided, we need to extract the feature vectors suitable for machine learning\n",
    "\n",
    "large variance in the results due to the short text!!\n",
    "\n",
    "curse of dimensionality \n",
    "\n",
    "\n",
    "### Extract significant features\n",
    "Bag of words \n",
    "Number of features is then the number of dictionnary of disctinc words of all the reviews. \n",
    "\" high-dimensional sparse datasets\"\n",
    "tokenizing words\n",
    "unigram model\n",
    "Regarding Step 1, we \n",
    "\n",
    "Feature selection\n",
    "The model may capture more information than required \n",
    "Computational complexity\n",
    "Overfitting if we have too many words, as it introduces nois!\n",
    "manual selection of important words?\n",
    "keep the frequency over a threshold, get rid of the most unfrequent words \n",
    "\n",
    "statistical approach, we do not reason in terms of occurence, rather in terms of frequency. \n",
    "\n",
    "Influence of vector feature on the accuracy\n",
    "\n",
    "but a word could be used many times in the same long review, but not very often in other reviews. So as to prevent this word from having an overweighted importance in the analsysis compared to others, we have to take into account the size of the review text. \n",
    "\n",
    "tfidf “Term Frequency times Inverse Document Frequency”\n",
    "\n",
    "\n",
    "Based on this, we can start searching for the best classifier! \n",
    "\n",
    "All along the lectures, we have encountered several methods which have different efficiencies in different situations.\n",
    "All methods are quite systematic as based on solid mathematics, but when using it on real data, here comes the empirical search for the best model, configuration, paramters if any.\n",
    "But we have tried to be very systematic in this empiric approach because making the experiment and results reproducible is a key principle of *Science*. And the purpose of the lecture and this project in particular is to study the *science* of data, i.e Data *Science*.\n",
    "\n",
    "## Tuning the models\n",
    "\n",
    "Empirical search is not absolute, but relative to the criteria choosen to assess the quality of the results. \n",
    "We have investigated different metrics base\n",
    "differents measures\n",
    "\n",
    "GridSearch \n",
    "\n",
    "\n",
    "## Assessing the results: metrics \n",
    "\n",
    "\n",
    "ROC curve, results are regarded as binary. \n",
    "\n",
    "# Sentiment Analysis \n",
    "\n",
    "Intuitively, the number of stars is positvely correlated with the presence of positive sentiment in the review. \n",
    "\n",
    "count vectorizer \n",
    "We cannot use bag-of-words anymore.\n",
    "\"not good\", \"not\" is a stop word that would be removed and \"good\" a positive adjective. The sentiment resulting from this analysis is the exact opposite of what it truly means. \n",
    "That is why we should consider pairs of words as features rather than single words. \n",
    "\n",
    "How to bring human understanding to the machine?\n",
    "By giving list of synonyms \n",
    "WordNet [9] is a lexical database, which contains the relations between similar words. The relations include synonym, hyponym, hypernym, and so on.\n",
    "measure easily how close a sentece, words are closed to the features = reduction of the number of features \n",
    "\n",
    "The text has several characteristics:\n",
    "\n",
    "1. subjectivity – whether the style of the sentence is subjective or objective;\n",
    "2. polarity – whether the author expresses positive or negative opinion.\n",
    "\n",
    "Limitation = ironic commentary \n",
    "“Item as described.\" neutral words often associated with good grade\n",
    " that purely contain implicit sentiments\n",
    "\n",
    "\"Sentiment categorization is essentially a classification problem, where features that contain opinions or sentiment information should be identified before the classification. For feature selection, Pang and Lee [5] suggested to remove objective sentences by extracting subjective ones.\"\n",
    "\n",
    "They proposed a text-categorization technique that is able to identify subjective content using minimum cut. Gann et al. [28] selected 6,799 tokens based on Twitter data, where each token is assigned a sentiment score, namely TSI(Total Sentiment Index), featuring itself as a positive token or a negative token. Specifically, a TSI for a certain token is computed as:\n",
    "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0015-2#CR27\n",
    "\n",
    "Word2vec\n",
    "\n",
    "The approach implements a bag-of-word model that simply counts the appearance of positive or negative (word) tokens for every sentence. If there are more positive tokens than negative ones, the sentence will be tagged as positive, and vice versa\n",
    "\n",
    "### How do we overcome the obstacle of the running time? \n",
    "The set used is composed of 96'000 samples \n",
    "work on a smaller dataset to save time and be more efficient \n",
    "Structure of the matrix, sparse\n",
    "counter, hashing \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONCLUSION\n",
    "\n",
    "Data Science, explore it on our own\n",
    "Pleasure to collaborate\n",
    "\n",
    "\n",
    "Ressources: \n",
    "[1]http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "[2]https://www.scss.tcd.ie/Khurshid.Ahmad/Research/Sentiments/K_Teams_Buchraest/mvie%20review%20review.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
